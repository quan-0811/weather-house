# =============================================================================
# WEATHER HOUSE DOCKER COMPOSE CONFIGURATION
# =============================================================================
# Purpose: Orchestrate all services for weather data pipeline
# Architecture: Kappa Architecture
#   Data → Kafka → Spark Structured Streaming → HDFS / Cassandra
#   Cassandra → Superset
#   HDFS: Bronze → Silver → Gold (via Spark batch jobs scheduled by Airflow)
# =============================================================================

version: '3.8'

# -----------------------------------------------------------------------------
# Networks
# -----------------------------------------------------------------------------
# Define custom network for all services to communicate
# All containers can reach each other by service name
networks:
  weather-network:
    driver: bridge
    name: weather-network

# -----------------------------------------------------------------------------
# Volumes
# -----------------------------------------------------------------------------
# Persistent storage for data that should survive container restarts
volumes:
  # Zookeeper data directory (cluster metadata, configuration)
  zookeeper-data:
    driver: local
  
  # Kafka data directories (message logs, one per broker)
  kafka-data-1:
    driver: local
  kafka-data-2:
    driver: local
  kafka-data-3:
    driver: local
  
  # HDFS data directories (file system data)
  hdfs-namenode-data:
    driver: local
  hdfs-datanode-data-1:
    driver: local
  hdfs-datanode-data-2:
    driver: local
  hdfs-datanode-data-3:
    driver: local
  hdfs-secondarynamenode-data:
    driver: local
  
  # Cassandra data directory (database files)
  cassandra-data:
    driver: local
  
  # PostgreSQL data directories (for Airflow and Superset metadata)
  postgres-airflow-data:
    driver: local
  postgres-superset-data:
    driver: local
  
  # Redis data (for caching, if needed)
  redis-data:
    driver: local

# -----------------------------------------------------------------------------
# Services
# -----------------------------------------------------------------------------
services:
  # ===========================================================================
  # ZOOKEEPER
  # ===========================================================================
  # Coordination service for Kafka cluster
  # Required for Kafka (not using KRaft mode)
  zookeeper:
    build:
      context: .
      dockerfile: zookeeper/Dockerfile
    container_name: zookeeper
    hostname: zookeeper
    networks:
      - weather-network
    ports:
      - "2181:2181"  # Client port
      - "2888:2888"  # Peer communication port
      - "3888:3888"  # Leader election port
    volumes:
      - zookeeper-data:/data
      - zookeeper-data:/datalog
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: "server.1=zookeeper:2888:3888;2181"
      ZOO_TICK_TIME: 2000
      ZOO_INIT_LIMIT: 10
      ZOO_SYNC_LIMIT: 5
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # ===========================================================================
  # KAFKA BROKERS (3 brokers for high availability)
  # ===========================================================================
  kafka-1:
    build:
      context: .
      dockerfile: kafka/Dockerfile
    container_name: kafka-1
    hostname: kafka-1
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "9092:9092"  # Broker 1 port
    volumes:
      - kafka-data-1:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 0
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  kafka-2:
    build:
      context: .
      dockerfile: kafka/Dockerfile
    container_name: kafka-2
    hostname: kafka-2
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "9093:9093"  # Broker 2 port
    volumes:
      - kafka-data-2:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9093"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  kafka-3:
    build:
      context: .
      dockerfile: kafka/Dockerfile
    container_name: kafka-3
    hostname: kafka-3
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "9094:9094"  # Broker 3 port
    volumes:
      - kafka-data-3:/var/lib/kafka/data
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9094"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # ===========================================================================
  # KAFKA TOPIC CREATOR (runs once to create topics)
  # ===========================================================================
  kafka-topics:
    build:
      context: .
      dockerfile: kafka/Dockerfile
    container_name: kafka-topics
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
    networks:
      - weather-network
    command: >
      bash -c "
      sleep 10 &&
      /usr/local/bin/create-topics.sh
      "
    restart: "no"  # Run once and exit

  # ===========================================================================
  # SPARK CLUSTER (1 master + 3 workers)
  # ===========================================================================
  spark-master:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark-master
    hostname: spark-master
    networks:
      - weather-network
    ports:
      - "18080:8080"  # Spark Master Web UI (mapped to 18080 to avoid conflict with Airflow)
      - "7077:7077"  # Spark Master RPC
      - "6066:6066"  # Spark Master REST API
    environment:
      SPARK_MODE: master
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  spark-worker-1:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "8081:8081"  # Spark Worker Web UI
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_WEBUI_PORT: 8081
    restart: unless-stopped

  spark-worker-2:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "8082:8081"  # Spark Worker Web UI (mapped to different host port)
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_WEBUI_PORT: 8081
    restart: unless-stopped

  spark-worker-3:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark-worker-3
    hostname: spark-worker-3
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "8083:8081"  # Spark Worker Web UI (mapped to different host port)
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_WEBUI_PORT: 8081
    restart: unless-stopped

  # ===========================================================================
  # HDFS CLUSTER (1 NameNode + 1 Secondary NameNode + 3 DataNodes)
  # ===========================================================================
  hdfs-namenode:
    build:
      context: .
      dockerfile: hdfs/Dockerfile
    container_name: hdfs-namenode
    hostname: namenode
    networks:
      - weather-network
    ports:
      - "9000:9000"  # NameNode RPC
      - "9870:9870"  # NameNode Web UI
    volumes:
      - hdfs-namenode-data:/hadoop/dfs/name
    environment:
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
      HDFS_NAMENODE_HTTP_PORT: 9870
    command: /start-namenode.sh
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  hdfs-secondarynamenode:
    build:
      context: .
      dockerfile: hdfs/Dockerfile
    container_name: hdfs-secondarynamenode
    hostname: secondarynamenode
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "9868:9868"  # Secondary NameNode Web UI
    volumes:
      - hdfs-secondarynamenode-data:/hadoop/dfs/secondary
    environment:
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
    command: /start-secondarynamenode.sh
    restart: unless-stopped

  hdfs-datanode-1:
    build:
      context: .
      dockerfile: hdfs/Dockerfile
    container_name: hdfs-datanode-1
    hostname: datanode-1
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "9864:9864"  # DataNode Web UI
    volumes:
      - hdfs-datanode-data-1:/hadoop/dfs/data
    environment:
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
      HDFS_DATANODE_PORT: 9866
      HDFS_DATANODE_HTTP_PORT: 9864
    command: /start-datanode.sh
    restart: unless-stopped

  hdfs-datanode-2:
    build:
      context: .
      dockerfile: hdfs/Dockerfile
    container_name: hdfs-datanode-2
    hostname: datanode-2
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "9865:9864"  # DataNode Web UI (mapped to different host port)
    volumes:
      - hdfs-datanode-data-2:/hadoop/dfs/data
    environment:
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
      HDFS_DATANODE_PORT: 9866
      HDFS_DATANODE_HTTP_PORT: 9864
    command: /start-datanode.sh
    restart: unless-stopped

  hdfs-datanode-3:
    build:
      context: .
      dockerfile: hdfs/Dockerfile
    container_name: hdfs-datanode-3
    hostname: datanode-3
    depends_on:
      hdfs-namenode:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "9866:9864"  # DataNode Web UI (mapped to different host port)
    volumes:
      - hdfs-datanode-data-3:/hadoop/dfs/data
    environment:
      HDFS_NAMENODE_HOST: namenode
      HDFS_NAMENODE_PORT: 9000
      HDFS_DATANODE_PORT: 9866
      HDFS_DATANODE_HTTP_PORT: 9864
    command: /start-datanode.sh
    restart: unless-stopped

  # ===========================================================================
  # CASSANDRA
  # ===========================================================================
  cassandra:
    build:
      context: .
      dockerfile: cassandra/Dockerfile
    container_name: cassandra
    hostname: cassandra
    networks:
      - weather-network
    ports:
      - "9042:9042"  # CQL native protocol
      - "7000:7000"  # Inter-node communication
    volumes:
      - cassandra-data:/var/lib/cassandra
    environment:
      CASSANDRA_CLUSTER_NAME: weather-cluster
      CASSANDRA_DC: datacenter1
      CASSANDRA_RACK: rack1
      CASSANDRA_ENDPOINT_SNITCH: GossipingPropertyFileSnitch
      CASSANDRA_SEEDS: cassandra
      CASSANDRA_LISTEN_ADDRESS: 0.0.0.0
      CASSANDRA_BROADCAST_ADDRESS: cassandra
      CASSANDRA_RPC_ADDRESS: 0.0.0.0
      CASSANDRA_BROADCAST_RPC_ADDRESS: cassandra
    healthcheck:
      test: ["CMD", "nodetool", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # ===========================================================================
  # POSTGRESQL (for Airflow and Superset metadata)
  # ===========================================================================
  postgres-airflow:
    image: postgres:15-alpine
    container_name: postgres-airflow
    hostname: postgres-airflow
    networks:
      - weather-network
    ports:
      - "5432:5432"
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  postgres-superset:
    image: postgres:15-alpine
    container_name: postgres-superset
    hostname: postgres-superset
    networks:
      - weather-network
    ports:
      - "5433:5432"  # Different host port to avoid conflict
    volumes:
      - postgres-superset-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ===========================================================================
  # REDIS (for Airflow and Superset caching)
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: redis
    hostname: redis
    networks:
      - weather-network
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ===========================================================================
  # AIRFLOW
  # ===========================================================================
  airflow-webserver:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: airflow-webserver
    hostname: airflow-webserver
    depends_on:
      postgres-airflow:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "8080:8080"  # Airflow Web UI (conflicts with Spark Master - use different host port if needed)
    volumes:
      - ./dags:/opt/airflow/dags
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow:5432/airflow
    command: >
      bash -c "
      airflow db upgrade &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
      airflow webserver
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    depends_on:
      postgres-airflow:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy
    networks:
      - weather-network
    volumes:
      - ./dags:/opt/airflow/dags
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow:5432/airflow
    command: airflow scheduler
    restart: unless-stopped

  # ===========================================================================
  # SUPERSET
  # ===========================================================================
  superset:
    build:
      context: .
      dockerfile: superset/Dockerfile
    container_name: superset
    hostname: superset
    depends_on:
      postgres-superset:
        condition: service_healthy
      cassandra:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - weather-network
    ports:
      - "8088:8088"  # Superset Web UI
    environment:
      SUPERSET_DATABASE_URI: postgresql://superset:superset@postgres-superset:5432/superset
      SUPERSET_REDIS_HOST: redis
      SUPERSET_REDIS_PORT: 6379
    command: >
      bash -c "
      superset db upgrade &&
      superset init &&
      gunicorn -w 4 -k gevent --timeout 300 -b 0.0.0.0:8088 superset.app:create_app()
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # ===========================================================================
  # WEATHER DATA PRODUCERS (3 instances for 3 Kafka partitions)
  # ===========================================================================
  producer-1:
    build:
      context: .
      dockerfile: ingestion/Dockerfile
    container_name: producer-1
    hostname: producer-1
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      kafka-topics:
        condition: service_completed_successfully
    networks:
      - weather-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9093,kafka-3:9094
      PRODUCER_ID: producer-1
      PRODUCER_INTERVAL_SECONDS: 60
    restart: unless-stopped

  producer-2:
    build:
      context: .
      dockerfile: ingestion/Dockerfile
    container_name: producer-2
    hostname: producer-2
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      kafka-topics:
        condition: service_completed_successfully
    networks:
      - weather-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9093,kafka-3:9094
      PRODUCER_ID: producer-2
      PRODUCER_INTERVAL_SECONDS: 60
    restart: unless-stopped

  producer-3:
    build:
      context: .
      dockerfile: ingestion/Dockerfile
    container_name: producer-3
    hostname: producer-3
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      kafka-topics:
        condition: service_completed_successfully
    networks:
      - weather-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9093,kafka-3:9094
      PRODUCER_ID: producer-3
      PRODUCER_INTERVAL_SECONDS: 60
    restart: unless-stopped

  # ===========================================================================
  # WEATHER DATA CONSUMERS (3 instances for 3 Kafka partitions)
  # ===========================================================================
  consumer-1:
    build:
      context: .
      dockerfile: src/streaming/Dockerfile
    container_name: consumer-1
    hostname: consumer-1
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      kafka-topics:
        condition: service_completed_successfully
    networks:
      - weather-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9093,kafka-3:9094
      KAFKA_GROUP_ID: weather-consumers
      CONSUMER_ID: consumer-1
    restart: unless-stopped

  consumer-2:
    build:
      context: .
      dockerfile: src/streaming/Dockerfile
    container_name: consumer-2
    hostname: consumer-2
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      kafka-topics:
        condition: service_completed_successfully
    networks:
      - weather-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9093,kafka-3:9094
      KAFKA_GROUP_ID: weather-consumers
      CONSUMER_ID: consumer-2
    restart: unless-stopped

  consumer-3:
    build:
      context: .
      dockerfile: src/streaming/Dockerfile
    container_name: consumer-3
    hostname: consumer-3
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
      kafka-3:
        condition: service_healthy
      kafka-topics:
        condition: service_completed_successfully
    networks:
      - weather-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9093,kafka-3:9094
      KAFKA_GROUP_ID: weather-consumers
      CONSUMER_ID: consumer-3
    restart: unless-stopped

